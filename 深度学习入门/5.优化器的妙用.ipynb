{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 深度学习中的优化器"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.梯度下降法（Batch Gradient Descent）\n",
    "&emsp;&emsp;在前面的章节中，我们解决了模型参数的更新问题：在每一轮的计算中，通过正向传播得到预测值，计算预测值与真实值之间的距离，反向更新模型中的参数，然后再进行正向传播。   \n",
    "&emsp;&emsp;在这个过程中，我们的目标是不断降低真实值与预测值之间的距离，所以我们通过求Loss对于待更新参数的偏导来作为参数更新的值，这种作法其实就是使用了优化器，这个优化器被称为标准梯度下降法(GD,Gradient Descent)。现在我们来详细介绍以下这个方法。  \n",
    "&emsp;&emsp;假设学习训练的模型参数为W，代价函数J(W),则代价函数关于模型参数的偏导数即相关梯度为$\\Delta J(W)$,学习率为$\\eta_t$,则使用梯度下降法更新参数的公式为：  \n",
    "<center>$W_{t+1} = W_t - \\eta_t \\Delta  J(W_t)$</center>\n",
    "其中，$W_t$表示t时刻的模型参数。  \n",
    "&emsp;&emsp;从表达式来看，模型参数的更新调整，与代价函数关于模型参数的梯度有关，即沿着梯度的方向不断减小模型参数，从而最小化代价函数。  \n",
    "&emsp;&emsp;基本策略可以理解为“在有限视距内寻找最快路径下山”，因此每走一步，参考当前位置最陡的方向进而迈出下一步。如下图：\n",
    "![avatar](img/gd.png) \n",
    "### 缺点  \n",
    "#### 训练速度慢：  \n",
    "&emsp;&emsp;每走一步都要要计算调整下一步的方向，下山的速度变慢。在应用于大型数据集中，每输入一个样本都要更新一次参数，且每次迭代都要遍历所有的样本。会使得训练过程及其缓慢，需要花费很长时间才能得到收敛解。  \n",
    "#### 容易陷入局部最优解：  \n",
    "&emsp;&emsp;由于是在有限视距内寻找下山的反向。当陷入平坦的洼地，会误以为到达了山地的最低点，从而不会继续往下走。所谓的局部最优解就是鞍点。落入鞍点，梯度为0，使得模型参数不在继续更新。  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.随机梯度下降(Stochastic Gradient Descent)  \n",
    "对比批量梯度下降法，假设从一批训练样本$n$中随机选取一个样本$i_s$。模型参数为$W$，代价函数为$J(W)$，梯度为$\\Delta J(W)$，学习率为$\\eta_t$，则使用随机梯度下降法更新参数表达式为：  \n",
    "<center>$W_{t+1}=W_t-\\eta_t g_t$</center>  \n",
    "其中，$g_t=\\Delta J_{i_s}(W_t; X^{(i_s)};X^{(i_s)}), \\ \\ i_s \\in \\{1,2,...,n\\}$表示随机选择的一个梯度方向，$W_t$表示$t$时刻的模型参数。  \n",
    "$E(g_t) = \\Delta J(W_t)$, 这里虽然引入了随机性和噪声，但期望仍然等于正确的梯度下降。  \n",
    "基本策略可以理解为随机梯度下降像是一个盲人下山，不用每走一步计算一次梯度，但是他总能下到山底，只不过过程会显得扭扭曲曲。  \n",
    "![avatar](img/bgd.png)\n",
    "#### 优点：  \n",
    "&emsp;&emsp;虽然SGD需要走很多步的样子，但是对梯度的要求很低（计算梯度快）。而对于引入噪声，大量的理论和实践工作证明，只要噪声不是特别大，SGD都能很好地收敛。  \n",
    "&emsp;&emsp;应用大型数据集时，训练速度很快。比如每次从百万数据样本中，取几百个数据点，算一个SGD梯度，更新一下模型参数。相比于标准梯度下降法的遍历全部样本，每输入一个样本更新一次参数，要快得多。  \n",
    "#### 缺点：  \n",
    "&emsp;&emsp;SGD在随机选择梯度的同时会引入噪声，使得权值更新的方向不一定正确。  \n",
    "&emsp;&emsp;此外，SGD也没能单独克服局部最优解的问题。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.批量梯度下降法（Mini-Batch Gradient Descent）\n",
    "&emsp;&emsp;假设批量训练样本总数为$n$，每次输入和输出的样本分别为$X^{(i)},Y^{(i)}$，模型参数为$W$，代价函数为$J(W)$，每输入一个样本$i$代价函数关于$W$的梯度为$\\Delta  J_i(W_t, X^{(i)}, Y^{(i)})$，学习率为$\\eta_t$，则使用批量梯度下降法更新参数表达式为：\n",
    "<center>$W_{t+1} = W_t - \\eta_t \\sum_{i=1}^n \\Delta J_i(W_t, X^{(i)}, Y^{(i)})$</center>  \n",
    "其中，$W_t$表示t时刻的模型参数。  \n",
    "&emsp;&emsp;从表达式来看，模型参数的调整更新与全部输入样本的代价函数的和（即批量/全局误差）有关。即每次权值调整发生在批量样本输入之后，而不是每输入一个样本就更新一次模型参数。这样就会大大加快训练速度。  \n",
    "&emsp;&emsp;基本策略可以理解为，在下山之前掌握了附近的地势情况，选择总体平均梯度最小的方向下山。  \n",
    "### 评价  \n",
    "&emsp;&emsp;批量梯度下降法比标准梯度下降法训练时间短，且每次下降的方向都很正确。  \n",
    "&emsp;&emsp;并没有解决其容易陷入鞍点的问题。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. 动量优化法\n",
    "&emsp;&emsp;使用动量(Momentum)的随机梯度下降法(SGD)，主要思想是引入一个积攒历史梯度信息动量来加速SGD。  \n",
    "&emsp;&emsp;从训练集中取一个大小为n的小批量$\\{X^{(1)}, X^{(2)},...,X^{(n)}\\}$样本，对应的真实值分别为$Y^{(i)}$，则Momentum优化表达式为：  \n",
    "<center>$\\left\\{ \\begin{aligned} & v_t = \\alpha v_{t-1} + \\eta_t \\Delta J(W_t, X^{(i_s)}, Y^{(i_s)}  ) \\\\ & W_{t+1} = W_t - v_t \\end{aligned} \\right.$</center>\n",
    "&emsp;&emsp;其中，$v_t$表示$t$时刻积攒的加速度。$\\alpha$表示动力的大小，一般取值为0.9（表示最大速度10倍于SGD）。$\\Delta J(W_t, X^{(i_s)}, Y^{(i_s)})$含义见SGD算法。$W_t$表示$t$时刻模型参数。其中$\\alpha v_{t-1}$所代表的就是Momentum，加入动量，可以使得梯度方向不变的维度上速度变快，梯度方向有所改变的维度上的更新速度变慢，这样就可以加快收敛并减小震荡。    \n",
    "&emsp;&emsp;理解策略为：由于当前权值的改变会受到上一次全职改变的影响，类似于小球向全局最低点滚动时带上了惯性，加快了小球向最低点滚动的速度，相当于小球从山上滚下来时是在盲目地沿着坡滚。\n",
    "![avatar](img/Momentum.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## 5. 动量优化法的改进\n",
    "&emsp;&emsp;牛顿加速梯度（NAG, Nesterov accelerated gradient）算法，是Momentum动量算法的变种。更新模型参数表达式如下：  \n",
    "<center>$\\left\\{ \\begin{aligned} & v_t = \\alpha v_{t-1} + \\eta_t \\Delta J(W_t - \\alpha v_{t-1}  ) \\\\ &W_{t+1} = W_t - v_t \\end{aligned} \\right.$</center>\n",
    "&emsp;&emsp;其中变量的含义与动量优化法完全相同，这种算法的做法是将$W_t - \\alpha v_{t-1}$近似的看作参数下一步会变成的值，那么计算梯度就不是再当前位置上，而是在未来近似出现的位置上。那么在梯度的优化策略上，就有了一定的预知能力！  \n",
    "&emsp;&emsp;理解策略为：为了让小球更快的滚到谷底，我们换了一个更加智能的小球，能够在接近上坡时，主动减速，避免震荡，能够提前知道它会去哪，应该去哪，并修正下降的路径。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. 自适应学习率优化算法\n",
    "&emsp;&emsp;自适应学习率优化算法针对于机器学习模型的学习率，传统的优化算法要么将学习率设置为常数要么根据训练次数调节学习率。极大忽视了学习率其他变化的可能性。然而，学习率对模型的性能有着显著的影响，因此需要采取一些策略来想办法更新学习率，从而提高训练速度。\n",
    "目前的自适应学习率优化算法主要有：AdaGrad算法，RMSProp算法，Adam算法以及AdaDelta算法。  \n",
    "此处已AdaGrad为例：  \n",
    "&emsp;&emsp;这个算法就可以对低频的参数做较大的更新，对高频的做较小的更新，也因此，对于稀疏的数据它的表现很好，很好地提高了 SGD 的鲁棒性，例如识别 Youtube 视频里面的猫，训练 GloVe word embeddings，因为它们都是需要在低频的特征上有更大的更新。  \n",
    "AdaGrad算法优化策略一般可以表示为：  \n",
    "<center>$W_{t+1}=W_t -\\frac{\\eta_0}{\\sqrt{\\sum_{t'=1}^t (g_{t',i})+\\epsilon}} \\odot g_{t,i}$</center>\n",
    "假定一个多分类问题，i表示第i个分类，t表示第t迭代同时也表示分类i累计出现的次数。$\\eta_0$表示初始的学习率取值一般为0.01，$\\epsilon$是一个取值很小的数（一般为1e-8）为了避免分母为0。Wt表示t时刻即第t迭代模型的参数，$g_{t,i}=\\Delta J(W_{t, i})$表示t时刻，指定分类i，代价函数$J(\\cdot)$关于W的梯度。  \n",
    "从表达式可以看出，对出现比较多的类别数据，Adagrad给予越来越小的学习率，而对于比较少的类别数据，会给予较大的学习率。因此Adagrad适用于数据稀疏或者分布不平衡的数据集。  \n",
    "Adagrad 的主要优势在于不需要人为的调节学习率，它可以自动调节；缺点在于，随着迭代次数增多，学习率会越来越小，最终会趋近于0。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. 优化器比较\n",
    "### 例一：\n",
    "![avatar](img/ksh.gif)\n",
    "改图描绘了在某一个曲面下，六种优化器的实际表现。\n",
    "#### 7.1.1. 速度  \n",
    "三个自适应学习优化器Adagrad、RMSProp与AdaDelta的下降速度明显比SGD要快，其中，Adagrad和RMSProp齐头并进，要比AdaDelta要快。  \n",
    "两个动量优化器Momentum和NAG由于刚开始走了岔路，初期下降的慢；随着慢慢调整，下降速度越来越快，其中NAG到后期甚至超过了领先的Adagrad和RMSProp。  \n",
    "#### 7.1.2. 下降轨迹  \n",
    "SGD和三个自适应优化器轨迹大致相同。两个动量优化器初期走了“岔路”，后期也调整了过来。  \n",
    "### 例二：\n",
    "![avatar](img/ksh2.gif)  \n",
    "#### 7.2.1. 速度 \n",
    "三个自适应学习率优化器没有进入鞍点，其中，AdaDelta下降速度最快，Adagrad和RMSprop则齐头并进。\n",
    "#### 7.2.2. 下降轨迹  \n",
    "两个动量优化器Momentum和NAG以及SGD都顺势进入了鞍点。但两个动量优化器在鞍点抖动了一会，就逃离了鞍点并迅速地下降，后来居上超过了Adagrad和RMSProp。  \n",
    "很遗憾，SGD进入了鞍点，却始终停留在了鞍点，没有再继续下降。  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8.优化器的选择\n",
    "&emsp;&emsp;那种优化器最好？该选择哪种优化算法？目前还没能够达达成共识。Schaul et al (2014)展示了许多优化算法在大量学习任务上极具价值的比较。虽然结果表明，具有自适应学习率的优化器表现的很鲁棒，不分伯仲，但是没有哪种算法能够脱颖而出。  \n",
    "&emsp;&emsp;如果数据是稀疏的，就用自适用方法，即 Adagrad, Adadelta, RMSprop, Adam。\n",
    "\n",
    "&emsp;&emsp;RMSprop, Adadelta, Adam 在很多情况下的效果是相似的。\n",
    "\n",
    "&emsp;&emsp;Adam 就是在 RMSprop 的基础上加了 bias-correction 和 momentum，\n",
    "\n",
    "&emsp;&emsp;随着梯度变的稀疏，Adam 比 RMSprop 效果会好。\n",
    "\n",
    "整体来讲，Adam 是最好的选择。\n",
    "\n",
    "&emsp;&emsp;很多论文里都会用 SGD，没有 momentum 等。SGD 虽然能达到极小值，但是比其它算法用的时间长，而且可能会被困在鞍点。\n",
    "\n",
    "如果需要更快的收敛，或者是训练更深更复杂的神经网络，需要用一种自适应的算法。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
