{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 神经网络中的激活函数"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.激活函数是干嘛的？"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&emsp;&emsp;首先我们要知道假如不使用激活函数，很容易验证，无论神经网络有多少层，单个神经元的输入和输出始终为线性函数，这就是最原始的感知机，其网络的逼近能力相当有限。为了解决这个问题，我们决定引入非线性函数作为激活函数，使深层神经网络表达能力更加强大。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.常用激活函数"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1Sigmoid函数  \n",
    "Sigmoid函数是一个并不被常用的非线性激活函数，其数学形式如下：  \n",
    "<font size=24><center>$f(z) = \\frac{1}{1+e^{-z}}$</center></font> \n",
    "Sigmoid函数的几何图像如下：  \n",
    "![avatar](img/Sigmoid.jpg)  \n",
    "该函数最大的用途是将输入数据z映射到(0,1)这个区间，也是最早期使用的激活函数之一。  \n",
    "Sigmoid函数的导函数及其分布情况如下：\n",
    "![avatar](img/Sigmoid'.jpg)\n",
    "![avatar](img/sig-moid'.jpg)\n",
    "作为一个激活函数，其缺点十分致命：\n",
    "1. 当z值非常大或者非常小时，通过上式我们可以看到，Sigmoid'()会无限接近于0，这会导致权重W的梯度接近0，使梯度的更新十分缓慢，造成梯度消失的现象。  \n",
    "2. 函数的输出不是以0为均值(即Sigmoid函数值域为(0,1))，这是不可取的，因为这会导致后一层的神经元将得到上一层输出的非0均值的信号作为输入。在反向传播时，对W求局部梯度会偏向于训练样本的分布方向，如均值x>0，那么W就会持续往正方向或负方向更新，是的收敛缓慢。\n",
    "主要应用场景：  \n",
    "Sigmoid函数用于隐藏层作为激活函数的话，很容易导致梯度消失现象，所以在一般情况下，隐藏层的激活函数一般不会选择Sigmoid函数，当当前任务为二分类任务时，可以选择Sigmoid函数作为输出层的激活函数，求二分类的概率。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 tanh函数\n",
    "在Sigmoid函数的基础上，对其缺点做出一定的改进，得到双曲正切函数(tanh函数)，其数学表示及分布情况为：  \n",
    "<font size=12><center>$f(z)=\\tanh(x)=\\frac{\\sinh(x)}{\\cosh(x)}=\\frac{e^x-e^{-x}}{e^x+e^{-x}}$</center></font>   \n",
    "![avatar](img/tanh.png)\n",
    "其导函数及分布情况为: \n",
    "<center>$f'(z)=1-(f(z))^2$</center>\n",
    "![avatar](img/tanh'.png)  \n",
    "tanh函数解决了Sigmoid函数输出均值非零的问题，但其并没有解决容易发生梯度消失的问题，在小规模的网络模型中，使用tanh可以降低计算量，但是网络层数增多后，就会导致收敛缓慢。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 ReLU函数\n",
    "ReLU函数是目前深度学习领域最常用的激活函数，其数学表示及分布情况为：  \n",
    "<font size=24><center>$f(x) = \\max(0, x)$</center></font>   \n",
    "![avatar](img/relu.png)\n",
    "其导函数分布情况为: \n",
    "![avatar](img/relu'.png)\n",
    "ReLU函数虽然看似只是一个简单的取最大值函数，但是却是近几年深度学习领域最为重要的成果之一，有以下几大优点：\n",
    "1. 解决了梯度消失问题。\n",
    "2. 计算速度非常快，只需要判断输入是否大于0即可。\n",
    "3. 收敛速度远远快于Sigmoid函数和tanh函数\n",
    "ReLU的缺点也十分明显：\n",
    "1. 同Sigmoid函数一样，均值非零。\n",
    "2. 当输入值小于0时，输出也为0，会使某些神经元永久性死亡，且该状态不可逆。这种情况可以通过调低学习率来改善。\n",
    "尽管存在这两种问题，但ReLU仍然是目前最常用的激活函数，在搭建人工神经网络的时候推介优先尝试。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 Leaky ReLU函数\n",
    "Leaky ReLU函数是为了解决ReLU函数中出现的神经元死亡的部分，将其数学表示及分布情况为：  \n",
    "<font size=24><center>$f(x) = \\max(ax, x)$</center></font>\n",
    "![avatar](img/lkrelu.png)\n",
    "Leaky ReLU函数将x<0的部分取了一个极小的斜率值，常见的$a$是取0.01，即$y=a*0.01$。\n",
    "这样当输入值x小于0时，对于y来说都不会等于0而造成神经元死亡。理论上来讲，leaky ReLU的所有优点，外加不会造成神经元死亡，但是实际使用中，并没有完全证明leaky ReLU总是好于ReLU。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.5 Softmax函数\n",
    "Softmax函数是深度学习分类任务中最为重要的激活函数，它能够将输入数据转换为多分类维度上的离散概率分布。其数学表示及可视化原理如下：\n",
    "<font size=24><center>$S_i = \\frac{e^i}{\\sum_j e^j}$</center></font>\n",
    "![avatar](img/softmax.jpg)\n",
    "利用Softmax()函数可以高效的完成多分类任务，常用于输出层。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.如何选择合适的激活函数\n",
    "目前并没有明确的规则来决定使用什么激活函数，仅能凭借经验：\n",
    "1. 深度学习往往需要大量时间来处理大量数据，模型的收敛速度是尤为重要的。所以，总体上来讲，训练深度学习网络尽量使用均值为0的数据(可以通过数据预处理来实现)作为输入和均值为0的数据作为输出。所以在选择时尽量选择具有上述性质的激活函数，可以加快模型的收敛速度。\n",
    "2. 如果使用ReLU作为激活函数，那么一定要小心设置learning rate，不要让网络出现半数以上的死亡神经元，假设确实解决不了，可以试试其他的激活函数。\n",
    "3. 最好不要用Sigmoid函数作为隐藏层的激活函数，可以试试tanh，但它的效果比不上ReLU。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
